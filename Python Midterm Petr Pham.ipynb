{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downloader: # 2. An independent class for `downloader` with appropriate functions and attributes (5pts)\n",
    "    '''\n",
    "    Downloader class for collection of data and storage of results.\n",
    "    '''\n",
    "    def __init__(self, allowLog = True):\n",
    "        '''\n",
    "        Initilization of Downloader object. Storing objects from the webpage with self.qwe, self.rty and self.uio\n",
    "        '''\n",
    "        self.allowLog = allowLog\n",
    "        self.qwe = [\"qwe\"]\n",
    "        self.rty = [\"rty\"]\n",
    "        self.uio = [\"uio\"]\n",
    "        if self.allowLog:\n",
    "            print('Downloader initialized.')\n",
    "    def savedf(self): # 4. `Pandas dataframe` with results saved as an attribute in the `downloader` object (5pts)\n",
    "        '''\n",
    "        Storing objects from the webpage as pandas DataFrame.\n",
    "        '''\n",
    "        df = {}\n",
    "        df['qwe'] = pd.DataFrame([x.characteristics for x in self.qwe])\n",
    "        df['rty'] = pd.DataFrame([x.characteristics for x in self.rty])\n",
    "        df['uio'] = pd.DataFrame([x.characteristics for x in self.uio])    \n",
    "        self.df = df\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IES_Web:\n",
    "    '''\n",
    "    Class containing methods for parsing IES websites from self.soup attribute.\n",
    "    It is designed as a parent class for specific pages\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,link):\n",
    "        self.link = link\n",
    "        r = requests.get(link)\n",
    "        r.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(r.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'link'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-08fe85f55f60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mIES_Web\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'link'"
     ]
    }
   ],
   "source": [
    "IES_Web()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(link)\n",
    "r.encoding = 'UTF-8'\n",
    "soup = BeautifulSoup(r.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper(object):\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.standardmedia.co.ke\"\n",
    "    def scrape_site(self):\n",
    "        res = requests.get(self.url)\n",
    "        html = BeautifulSoup(res.content, 'html.parser')\n",
    "        if html:\n",
    "            div = html.find(\"div\", class_=\"col-xs-6 col-md-6 zero\")\n",
    "            ul = div.find_all(\"ul\", class_=\"sub-stories-2\")\n",
    "            data = []\n",
    "            for item in ul:\n",
    "                img_url = item.find(\"img\").get(\"src\")\n",
    "                text = item.find(\"img\").get(\"alt\")\n",
    "                link = item.find(\"li\").find(\"a\").get(\"href\")\n",
    "                data.append({'title': text,\n",
    "                  'link': link,\n",
    "                  'img': img_url\n",
    "                    })\n",
    "        return json.dumps(data, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-49d8012d5dc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscraper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscrape_site\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-79-ff2a69f638d1>\u001b[0m in \u001b[0;36mscrape_site\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"col-xs-6 col-md-6 zero\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mul\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sub-stories-2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mul\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "scraper.scrape_site()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
